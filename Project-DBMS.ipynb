{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1933e632-472a-4bbf-b325-b1397eafc27e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %%sh\n",
    "# /databricks/python/bin/pip install --upgrade pip\n",
    "# /databricks/python/bin/pip install --upgrade pymongo\n",
    "# /databricks/python/bin/pip install --upgrade pyspark\n",
    "# /databricks/python/bin/pip install --upgrade databricks-spark-xml\n",
    "# /databricks/python/bin/pip install --upgrade pymongo-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3627505d-9f2d-4699-904e-728569d59ddb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871e809b-3cc4-4056-8509-ab40b0520501",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from Cosmos DB\") \\\n",
    "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "uri = 'mongodb://project-db:gGMORZyxSMjSQ2cWRhVbOJSOkYOFvauXA6YoYxr1MQrJKVm6WH7MU6QSCQ1D0zxODAtnYhb1vzwVACDbJ524Sg==@project-db.mongo.cosmos.azure.com:10255/?ssl=true&retrywrites=false&replicaSet=globaldb&maxIdleTimeMS=120000&appName=@project-db@'\n",
    "database_name = 'project-database'\n",
    "\n",
    "\n",
    "\n",
    "# Load the MongoDB collections into PySpark DataFrames\n",
    "listing_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option('database',database_name).option(\"collection\", \"listings_main\").load()\n",
    "property_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option('database',database_name).option(\"collection\", \"property_collection\").load()\n",
    "reviews_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option('database',database_name).option(\"collection\", \"reviews\").load()\n",
    "rsummary_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option('database',database_name).option(\"collection\", \"previews_summary_collection\").load()\n",
    "\n",
    "cal_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", database_name).option(\"collection\", \"cal\").load()\n",
    "neighbourhoods_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", database_name).option(\"collection\", \"neighbourhoods\").load()\n",
    "host_collection_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", database_name).option(\"collection\", \"host_collection\").load()\n",
    "availiability_collection_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", database_name).option(\"collection\", \"availiability_collection\").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c30926b8-3fd6-46d9-b7d8-f2b9549118b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the DataFrames as temporary views\n",
    "listing_df.createOrReplaceTempView(\"listings_main\")\n",
    "property_df.createOrReplaceTempView(\"property_collection\")\n",
    "reviews_df.createOrReplaceTempView(\"reviews\")\n",
    "rsummary_df.createOrReplaceTempView(\"previews_summary_collection\")\n",
    "cal_df.createOrReplaceTempView(\"cal\")\n",
    "neighbourhoods_df.createOrReplaceTempView(\"neighbourhoods\")\n",
    "host_collection_df.createOrReplaceTempView(\"host_collection\")\n",
    "availiability_collection_df.createOrReplaceTempView(\"availiability_collection\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873e3317-068c-4496-851a-ef384bf87e6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUERY 1: Display list of stays in Portland, OR with details: name, neighbourhood, room type, how many guests it accommodates, property type and amenities, per night’s cost and is available for the next two days in descending order of rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6558654e-eda5-4c7f-8184-c4d8f0b7f550",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-2580591174032057>:41\u001b[0m\n",
       "\u001b[1;32m     38\u001b[0m result \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(query)\n",
       "\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n",
       "\u001b[0;32m---> 41\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n",
       "\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n",
       "\u001b[1;32m    915\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_A_BOOLEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m    916\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n",
       "\u001b[1;32m    917\u001b[0m     )\n",
       "\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n",
       "\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1729.showString.\n",
       ": com.mongodb.MongoExecutionTimeoutException: Request timed out. Retries due to rate limiting: True.\n",
       "\tat com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:239)\n",
       "\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:171)\n",
       "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:302)\n",
       "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:258)\n",
       "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n",
       "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:500)\n",
       "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
       "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:224)\n",
       "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:202)\n",
       "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:118)\n",
       "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:110)\n",
       "\tat com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:343)\n",
       "\tat com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:334)\n",
       "\tat com.mongodb.internal.operation.CommandOperationHelper.executeCommandWithConnection(CommandOperationHelper.java:220)\n",
       "\tat com.mongodb.internal.operation.CommandOperationHelper$5.call(CommandOperationHelper.java:206)\n",
       "\tat com.mongodb.internal.operation.OperationHelper.withReadConnectionSource(OperationHelper.java:462)\n",
       "\tat com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:203)\n",
       "\tat com.mongodb.internal.operation.AggregateOperationImpl.execute(AggregateOperationImpl.java:189)\n",
       "\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:296)\n",
       "\tat com.mongodb.internal.operation.CountOperation.execute(CountOperation.java:257)\n",
       "\tat com.mongodb.internal.operation.CountOperation.execute(CountOperation.java:64)\n",
       "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:190)\n",
       "\tat com.mongodb.client.internal.MongoCollectionImpl.executeCount(MongoCollectionImpl.java:222)\n",
       "\tat com.mongodb.client.internal.MongoCollectionImpl.countDocuments(MongoCollectionImpl.java:191)\n",
       "\tat com.mongodb.client.internal.MongoCollectionImpl.countDocuments(MongoCollectionImpl.java:186)\n",
       "\tat com.mongodb.spark.rdd.partitioner.MongoSamplePartitioner.$anonfun$partitions$6(MongoSamplePartitioner.scala:88)\n",
       "\tat com.mongodb.spark.rdd.partitioner.MongoSamplePartitioner.$anonfun$partitions$6$adapted(MongoSamplePartitioner.scala:88)\n",
       "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
       "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
       "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
       "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
       "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
       "\tat com.mongodb.spark.rdd.partitioner.MongoSamplePartitioner.partitions(MongoSamplePartitioner.scala:88)\n",
       "\tat com.mongodb.spark.rdd.partitioner.DefaultMongoPartitioner.partitions(DefaultMongoPartitioner.scala:34)\n",
       "\tat com.mongodb.spark.rdd.MongoRDD.getPartitions(MongoRDD.scala:135)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n",
       "\tat org.apache.spark.ShuffleDependency.<init>(Dependency.scala:127)\n",
       "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:494)\n",
       "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:258)\n",
       "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:252)\n",
       "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleId(ShuffleExchangeExec.scala:230)\n",
       "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$relationFuture$2(ShuffleExchangeExec.scala:92)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:108)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:334)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:330)\n",
       "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$relationFuture$1(ShuffleExchangeExec.scala:92)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:475)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:475)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:474)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withOptimisticTransaction(SQLExecution.scala:493)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:473)\n",
       "\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-2580591174032057>:41\u001b[0m\n\u001b[1;32m     38\u001b[0m result \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(query)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    915\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_A_BOOLEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    916\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    917\u001b[0m     )\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1729.showString.\n: com.mongodb.MongoExecutionTimeoutException: Request timed out. Retries due to rate limiting: True.\n\tat com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:239)\n\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:171)\n\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:302)\n\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:258)\n\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:500)\n\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:224)\n\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:202)\n\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:118)\n\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:110)\n\tat com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:343)\n\tat com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:334)\n\tat com.mongodb.internal.operation.CommandOperationHelper.executeCommandWithConnection(CommandOperationHelper.java:220)\n\tat com.mongodb.internal.operation.CommandOperationHelper$5.call(CommandOperationHelper.java:206)\n\tat com.mongodb.internal.operation.OperationHelper.withReadConnectionSource(OperationHelper.java:462)\n\tat com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:203)\n\tat com.mongodb.internal.operation.AggregateOperationImpl.execute(AggregateOperationImpl.java:189)\n\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:296)\n\tat com.mongodb.internal.operation.CountOperation.execute(CountOperation.java:257)\n\tat com.mongodb.internal.operation.CountOperation.execute(CountOperation.java:64)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:190)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeCount(MongoCollectionImpl.java:222)\n\tat com.mongodb.client.internal.MongoCollectionImpl.countDocuments(MongoCollectionImpl.java:191)\n\tat com.mongodb.client.internal.MongoCollectionImpl.countDocuments(MongoCollectionImpl.java:186)\n\tat com.mongodb.spark.rdd.partitioner.MongoSamplePartitioner.$anonfun$partitions$6(MongoSamplePartitioner.scala:88)\n\tat com.mongodb.spark.rdd.partitioner.MongoSamplePartitioner.$anonfun$partitions$6$adapted(MongoSamplePartitioner.scala:88)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.rdd.partitioner.MongoSamplePartitioner.partitions(MongoSamplePartitioner.scala:88)\n\tat com.mongodb.spark.rdd.partitioner.DefaultMongoPartitioner.partitions(DefaultMongoPartitioner.scala:34)\n\tat com.mongodb.spark.rdd.MongoRDD.getPartitions(MongoRDD.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n\tat org.apache.spark.ShuffleDependency.<init>(Dependency.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:494)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:258)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:252)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleId(ShuffleExchangeExec.scala:230)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$relationFuture$2(ShuffleExchangeExec.scala:92)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:108)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:334)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:330)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$relationFuture$1(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:475)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:475)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:474)\n\tat org.apache.spark.sql.execution.SQLExecution$.withOptimisticTransaction(SQLExecution.scala:493)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:473)\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "com.mongodb.MongoExecutionTimeoutException: Request timed out. Retries due to rate limiting: True.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## QUESTION 1 - Not able to execute due to resource constraints\n",
    "\n",
    "# Define the SQL query\n",
    "query = \"\"\"\n",
    "    SELECT l.name, l.neighbourhood, p.room_type, p.accommodates,\n",
    "           p.property_type, p.amenities, r.review_scores_rating, c.price\n",
    "    FROM listings_main l\n",
    "    LEFT JOIN property_collection p ON l.id = p.id\n",
    "    LEFT JOIN previews_summary_collection r ON l.id = r.id\n",
    "    LEFT JOIN cal c ON c.listing_id = l.id\n",
    "    WHERE l.city_number = 2 \n",
    "      AND c.available = 't'\n",
    "      AND (c.date = DATE_ADD(CURRENT_DATE(), 1) OR c.date = DATE_ADD(CURRENT_DATE(), 2))\n",
    "    ORDER BY r.review_scores_rating DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "result.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6566b6-b809-4d51-b84d-5a8ccb939447",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Q1 multiple queries\n",
    "query1 = \"\"\"\n",
    "    SELECT l.id, l.name, l.neighbourhood, p.room_type, p.accommodates, p.property_type, p.amenities\n",
    "    FROM listings_main l\n",
    "    LEFT JOIN property_collection p ON l.id = p.id\n",
    "    WHERE l.city_number = 2\n",
    "\"\"\"\n",
    "result1 = spark.sql(query1)\n",
    "\n",
    "# Create temporary view for result1\n",
    "result1.createOrReplaceTempView(\"result1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f933d7-70f4-43f9-b63a-1743a466a82a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query2 = \"\"\"\n",
    "    SELECT r.id, r.name, r.neighbourhood, r.room_type, r.accommodates, r.property_type, r.amenities,\n",
    "           s.review_scores_rating\n",
    "    FROM result1 r\n",
    "    LEFT JOIN previews_summary_collection s ON r.id = s.id\n",
    "\"\"\"\n",
    "result2 = spark.sql(query2)\n",
    "\n",
    "# Create temporary view for result2\n",
    "result2.createOrReplaceTempView(\"result2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70cad82-a083-40d8-8aa8-0db04c87df09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "tomorrow = date.today() + timedelta(days=1)\n",
    "day_after_tomorrow = date.today() + timedelta(days=2)\n",
    "\n",
    "query3 = f\"\"\"\n",
    "    SELECT c.id, c.name, c.neighbourhood, c.room_type, c.accommodates, c.property_type, c.amenities,\n",
    "           c.review_scores_rating, d.price\n",
    "    FROM result2 c\n",
    "    LEFT JOIN cal d ON c.id = d.listing_id\n",
    "    WHERE d.available = 't'\n",
    "      AND (d.date = '{tomorrow}' OR d.date = '{day_after_tomorrow}')\n",
    "\"\"\"\n",
    "result3 = spark.sql(query3)\n",
    "\n",
    "# Create temporary view for result3\n",
    "result3.createOrReplaceTempView(\"result3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5557d33e-a902-48a0-bd3c-8105d0703167",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+---------------+------------+--------------------+--------------------+--------------------+-------+\n",
      "|                id|                name|       neighbourhood|      room_type|accommodates|       property_type|           amenities|review_scores_rating|  price|\n",
      "+------------------+--------------------+--------------------+---------------+------------+--------------------+--------------------+--------------------+-------+\n",
      "|          43595828|New NE PDX Garden...|Portland, Oregon,...|Entire home/apt|           3|     Entire bungalow|[\"Carbon monoxide...|                 5.0|$500.00|\n",
      "|          39643910|Simple convenient...|                    |   Private room|           1|Private room in home|[\"Carbon monoxide...|                 5.0| $32.00|\n",
      "|          27031951|The Goddess Room ...|Portland, Oregon,...|   Private room|           2|Private room in home|[\"Carbon monoxide...|                 5.0|$200.00|\n",
      "|563954648920832614|Executive 1b unit...|Portland, Oregon,...|Entire home/apt|           2|  Entire rental unit|[\"Carbon monoxide...|                 5.0| $86.00|\n",
      "|          32900769|BIG monthly Disco...|Portland, Oregon,...|Entire home/apt|           6|         Entire home|[\"Drying rack for...|                 5.0|$210.00|\n",
      "|          39643910|Simple convenient...|                    |   Private room|           1|Private room in home|[\"Carbon monoxide...|                 5.0| $30.00|\n",
      "|          46795705|Modern luxury nea...|                    |Entire home/apt|           4|         Entire home|[\"Carbon monoxide...|                 5.0|$400.00|\n",
      "|          40094761|Northeast Portlan...|Portland, Oregon,...|Entire home/apt|           2|         Entire home|[\"Carbon monoxide...|                 5.0|$200.00|\n",
      "|          31539235|Stunning Modern L...|Portland, Oregon,...|Entire home/apt|           2|         Entire loft|[\"Carbon monoxide...|                 5.0|$149.00|\n",
      "|          40864982|Amazing location,...|                    |Entire home/apt|          15|         Entire home|[\"Carbon monoxide...|                 5.0|$874.00|\n",
      "|          27031951|The Goddess Room ...|Portland, Oregon,...|   Private room|           2|Private room in home|[\"Carbon monoxide...|                 5.0|$200.00|\n",
      "|          16146098|Cozy Private Dwel...|Portland, Oregon,...|Entire home/apt|           2|  Entire guest suite|[\"Carbon monoxide...|                 5.0|$110.00|\n",
      "|          48231531|Hawthorne Heights...|Portland, Oregon,...|Entire home/apt|           5|  Entire guest suite|[\"Carbon monoxide...|                 5.0|$213.00|\n",
      "|          40864982|Amazing location,...|                    |Entire home/apt|          15|         Entire home|[\"Carbon monoxide...|                 5.0|$874.00|\n",
      "|          23609556|Bright House-shar...|Portland, Oregon,...|   Private room|           4|Private room in home|[\"Carbon monoxide...|                 5.0|$100.00|\n",
      "|          21517694|Bright Contempora...|Portland, Oregon,...|Entire home/apt|           4|        Entire condo|[\"Carbon monoxide...|                 5.0|$135.00|\n",
      "|          51907687|3 bedroom family ...|Portland, Oregon,...|Entire home/apt|           5|         Entire home|[\"Carbon monoxide...|                 5.0| $95.00|\n",
      "|          44325859|Room in Stylish M...|Portland, Oregon,...|   Private room|           1|Private room in home|[\"Carbon monoxide...|                 5.0| $80.00|\n",
      "|          32423425|\"THE 2509\"  Creat...|Portland, Oregon,...|Entire home/apt|           2|  Entire rental unit|[\"Carbon monoxide...|                 5.0|$120.00|\n",
      "|          24916586|CDC Compliant | R...|Portland, Oregon,...|Entire home/apt|           4|         Entire home|[\"Drying rack for...|                 5.0|$129.00|\n",
      "+------------------+--------------------+--------------------+---------------+------------+--------------------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query4 = \"\"\"\n",
    "    SELECT *\n",
    "    FROM result3\n",
    "    ORDER BY review_scores_rating DESC\n",
    "\"\"\"\n",
    "result4 = spark.sql(query4)\n",
    "\n",
    "# Display the result\n",
    "result4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032f6425-f512-497b-80a8-8e2ad49056b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUERY 2: Are there any neighbourhoods in any of the cities that don’t have any listings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0f0e73-f56d-4dbd-b0c1-0c9554bcba50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       neighbourhood|\n",
      "+--------------------+\n",
      "|     Harvard Heights|\n",
      "|           Mar Vista|\n",
      "|              Sunbow|\n",
      "| Chesterfield Square|\n",
      "|          West Hills|\n",
      "|       Angeles Crest|\n",
      "|Northwest Antelop...|\n",
      "|       Glassell Park|\n",
      "|           Hollywood|\n",
      "|Sellwood-Moreland...|\n",
      "|            Bradbury|\n",
      "|             Maywood|\n",
      "|     Tujunga Canyons|\n",
      "|          Mount Hope|\n",
      "|            Oak Park|\n",
      "|  Rancho Penasquitos|\n",
      "|             Compton|\n",
      "|         Culver City|\n",
      "|              Lomita|\n",
      "|              Lennox|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "        SELECT Distinct neighbourhood FROM neighbourhoods WHERE neighbourhood NOT IN (SELECT neighbourhood from listings_main)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "result.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1583144b-782d-4153-85cb-aeff318f0457",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUERY 3: For “Entire home/apt” type listings in Portland provide it’s availability estimate for each month of Spring and Winter this year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc29ade4-a396-4d77-9f38-58fc99af8e08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+\n",
      "|month(date)|count_t|count_f|\n",
      "+-----------+-------+-------+\n",
      "|         12|  53428|  65147|\n",
      "|          3|   9520|  24905|\n",
      "|          5|  68983|  49592|\n",
      "|          4|  53080|  61670|\n",
      "+-----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "        SELECT MONTH(c.date), \n",
    "       SUM(CASE WHEN c.available = 't' THEN 1 ELSE 0 END) AS count_t,\n",
    "       SUM(CASE WHEN c.available = 'f' THEN 1 ELSE 0 END) AS count_f\n",
    "        FROM listings_main l\n",
    "        LEFT JOIN property_collection p ON l.id = p.id\n",
    "        LEFT JOIN cal c on c.listing_id = l.id\n",
    "        WHERE p.room_type = 'Entire home/apt' AND l.city_number = '2' AND MONTH(date) IN (12,1,2,3,4,5) AND YEAR(date) = 2023\n",
    "        GROUP BY MONTH(c.date)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "result.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c740b3d-43e3-46e9-a13b-78011d6049b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUERY 4: For each city, how many reviews are received for December of each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "733cabc4-2663-4743-8dec-261d6437c229",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+\n",
      "|city_number|year(date)|count(1)|\n",
      "+-----------+----------+--------+\n",
      "|          1|      2011|      86|\n",
      "|          1|      2021|   21755|\n",
      "|          1|      2014|    1374|\n",
      "|          1|      2019|   17182|\n",
      "|          1|      2016|    4868|\n",
      "|          1|      2010|      13|\n",
      "|          1|      2015|    3112|\n",
      "|          1|      2022|   27064|\n",
      "|          1|      2017|    8214|\n",
      "|          1|      2018|   12900|\n",
      "|          1|      2012|     233|\n",
      "|          1|      2020|    9250|\n",
      "|          2|      2014|     354|\n",
      "|          3|      2017|      71|\n",
      "|          3|      2018|     132|\n",
      "|          4|      2021|   11145|\n",
      "|          2|      2016|    1450|\n",
      "|          4|      2019|    6910|\n",
      "|          3|      2015|       8|\n",
      "|          2|      2022|    5269|\n",
      "+-----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT l.city_number, YEAR(r.date), count(*) FROM listings_main l\n",
    "        LEFT JOIN reviews r ON r.listing_id = l.id\n",
    "        WHERE MONTH(r.date) = 12\n",
    "        GROUP BY l.city_number, YEAR(r.date)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad9405c-d724-41e0-a40d-8e33554a4510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUERY 5: Retrieve host-related information and calculate the percentage of five-star listings for each host, based on the total number of listings. Provides insights into host performance and the proportion of highly rated listings they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030ea88c-8575-4f55-9f51-45d081ebfaea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+------------------+-------------------------+-----------------+----------+-----------------------+\n",
      "|  host_id|host_acceptance_rate|host_response_rate|host_response_time|host_total_listings_count|host_is_superhost|host_since|FiveStarListingsPercent|\n",
      "+---------+--------------------+------------------+------------------+-------------------------+-----------------+----------+-----------------------+\n",
      "| 33451922|                  0%|              100%|within a few hours|                        1|                f|2015-05-15|                    1.0|\n",
      "|172746749|                 N/A|               N/A|               N/A|                        1|                f|2018-02-10|                    1.0|\n",
      "|486200100|                100%|               80%|      within a day|                        1|                f|2022-11-03|                    1.0|\n",
      "| 50241674|                 N/A|               N/A|               N/A|                        1|                f|2015-11-30|                    1.0|\n",
      "|  9995385|                 N/A|               N/A|               N/A|                        1|                f|2013-11-13|                    1.0|\n",
      "|496056591|                100%|              100%|    within an hour|                        1|                f|2023-01-14|                    1.0|\n",
      "| 10609520|                 N/A|               N/A|               N/A|                        1|                f|2013-12-13|                    1.0|\n",
      "|389360913|                100%|               N/A|               N/A|                        1|                f|2021-02-20|                    1.0|\n",
      "| 14565406|                100%|              100%|    within an hour|                        1|                f|2014-04-21|                    1.0|\n",
      "|491213194|                 71%|              100%|within a few hours|                        1|                f|2022-12-12|                    1.0|\n",
      "| 58134238|                 42%|                0%|a few days or more|                        1|                f|2016-02-10|                    1.0|\n",
      "|188146943|                 83%|              100%|    within an hour|                        1|                f|2018-05-06|                    1.0|\n",
      "| 17424690|                100%|              100%|within a few hours|                        1|                t|2014-06-29|                    1.0|\n",
      "|141722246|                100%|               N/A|               N/A|                        1|                f|2017-07-21|                    1.0|\n",
      "|  4327399|                 96%|              100%|    within an hour|                        1|                t|2012-12-04|                    1.0|\n",
      "| 50312673|                 N/A|               N/A|               N/A|                        1|                f|2015-12-01|                    1.0|\n",
      "| 86351797|                100%|              100%|    within an hour|                        1|                t|2016-07-27|                    1.0|\n",
      "| 17532979|                 N/A|               N/A|               N/A|                        1|                f|2014-07-02|                    1.0|\n",
      "|445743078|                 96%|              100%|    within an hour|                        1|                t|2022-02-18|                    1.0|\n",
      "|393195385|                 50%|              100%|      within a day|                        1|                f|2021-03-18|                    1.0|\n",
      "+---------+--------------------+------------------+------------------+-------------------------+-----------------+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "    SELECT hc.host_id, hc.host_acceptance_rate, hc.host_response_rate, hc.host_response_time, hc.host_total_listings_count, hc.host_is_superhost, hc.host_since,\n",
    "    fiveStars.NumberOfFiveStarListings/hc.host_total_listings_count AS FiveStarListingsPercent \n",
    "    FROM host_collection hc, (\n",
    "    SELECT h.host_id, COUNT(*) as NumberOfFiveStarListings FROM listings_main l\n",
    "    LEFT JOIN host_collection h ON h.id =  l.id\n",
    "    LEFT JOIN previews_summary_collection r ON l.id = r.id\n",
    "    WHERE r.review_scores_rating = 5\n",
    "    GROUP BY host_id) fiveStars\n",
    "    Where fiveStars.host_id = hc.host_id\n",
    "    ORDER BY FiveStarListingsPercent DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34861f1e-e194-4324-9678-4c946338aa60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1c99ce-74fd-4034-b537-819e623f85bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01c524d-0147-4570-bf52-7c9336e18ef4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+------------+--------------------+--------------------+--------------------+\n",
      "|                name|       neighbourhood|      room_type|accommodates|       property_type|           amenities|review_scores_rating|\n",
      "+--------------------+--------------------+---------------+------------+--------------------+--------------------+--------------------+\n",
      "|Doug Fir Room/Pet...|Portland, Oregon,...|     Hotel room|           2|Room in boutique ...|[\"Carbon monoxide...|                 5.0|\n",
      "|Willamette River ...|                    |Entire home/apt|           2|        Entire condo|[\"Shampoo\", \"Esse...|                 5.0|\n",
      "|Bright and Welcom...|Portland, Oregon,...|Entire home/apt|           2|  Entire guest suite|[\"Carbon monoxide...|                 5.0|\n",
      "|Jojo’s Blue House...|Portland, Oregon,...|Entire home/apt|          10|         Entire home|[\"Drying rack for...|                 5.0|\n",
      "|Alberta Arts and ...|                    |Entire home/apt|           5|         Entire home|[\"Shampoo\", \"Esse...|                 5.0|\n",
      "|The Sparrow on Pi...|Portland, Oregon,...|Entire home/apt|           5|         Entire home|[\"Carbon monoxide...|                 5.0|\n",
      "|Quiet Neighborhoo...|                    |Entire home/apt|           6|         Entire home|[\"Carbon monoxide...|                 5.0|\n",
      "|Charming sunny 2 ...|Portland, Oregon,...|Entire home/apt|           3|         Entire home|[\"Drying rack for...|                 5.0|\n",
      "|Bright + Cozy Gue...|Portland, Oregon,...|Entire home/apt|           3|   Entire guesthouse|[\"Carbon monoxide...|                 5.0|\n",
      "|Irving Parlor (gu...|Portland, Oregon,...|Entire home/apt|           2|   Entire guesthouse|[\"Carbon monoxide...|                 5.0|\n",
      "|Entire Main Floor...|Portland, Oregon,...|Entire home/apt|           4|     Entire bungalow|[\"Drying rack for...|                 5.0|\n",
      "|Historic Jacobber...|Portland, Oregon,...|Entire home/apt|           4|         Entire home|[\"Carbon monoxide...|                 5.0|\n",
      "|Cozy and Bright R...|Portland, Oregon,...|Entire home/apt|           5|         Entire home|[\"Carbon monoxide...|                 5.0|\n",
      "|Bamboo Room/Petit...|Portland, Oregon,...|     Hotel room|           2|Room in boutique ...|[\"Carbon monoxide...|                 5.0|\n",
      "|Beautiful, Contem...|Portland, Oregon,...|Entire home/apt|           4|   Entire guesthouse|[\"Carbon monoxide...|                 5.0|\n",
      "|Cozy Bedroom with...|                    |   Private room|           1|Private room in home|[\"Carbon monoxide...|                 5.0|\n",
      "|Listen to Records...|Portland, Oregon,...|Entire home/apt|           3|   Entire guesthouse|[\"Carbon monoxide...|                 5.0|\n",
      "|The Pinnacle in T...|                    |Entire home/apt|           2|        Entire condo|[\"Drying rack for...|                 5.0|\n",
      "|Shared retreat in...|Portland, Oregon,...|   Private room|           2|Private room in home|[\"Carbon monoxide...|                 5.0|\n",
      "|Harlow Hotel, Exe...|                    |   Private room|           2|Room in boutique ...|[\"Shampoo\", \"Esse...|                 5.0|\n",
      "+--------------------+--------------------+---------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Define the SQL query\n",
    "# query = \"\"\"\n",
    "#     SELECT l.name, l.neighbourhood, p.room_type, p.accommodates,\n",
    "#            p.property_type, p.amenities, r.review_scores_rating\n",
    "#     FROM listings_main l\n",
    "#     LEFT JOIN property_collection p ON l.id = p.id\n",
    "#     LEFT JOIN previews_summary_collection r ON l.id = r.id\n",
    "#     WHERE l.city_number = 2\n",
    "#     ORDER BY r.review_scores_rating DESC\n",
    "# \"\"\"\n",
    "\n",
    "# # Execute the SQL query\n",
    "# result = spark.sql(query)\n",
    "\n",
    "# # Display the result\n",
    "# result.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project-DBMS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
